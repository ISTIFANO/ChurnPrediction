import streamlit as st
import os
import sys
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
import logging

# -------------------------------
# Logging
# -------------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -------------------------------
# Path du mod√®le
# -------------------------------
def get_model_path():
    """Retourne le chemin absolu vers le mod√®le"""
    current_dir = Path(__file__).parent  # src/interfaces/
    src_dir = current_dir.parent  # src/
    model_path = src_dir / "domain" / "models" / "best_pipeline_model"
    return model_path.resolve()  # absolu

# -------------------------------
# Setup environnement PySpark
# -------------------------------
def setup_environment():
    python_exe = sys.executable
    os.environ["PYSPARK_PYTHON"] = python_exe
    os.environ["PYSPARK_DRIVER_PYTHON"] = python_exe

# -------------------------------
# Initialisation Spark
# -------------------------------
@st.cache_resource
def initialize_spark():
    try:
        spark = SparkSession.builder \
            .appName("ChurnPredictionApp") \
            .master("local[1]") \
            .config("spark.driver.memory", "2g") \
            .getOrCreate()
        spark.sparkContext.setLogLevel("ERROR")
        logger.info("‚úÖ Spark session initialized")
        return spark
    except Exception as e:
        logger.error(f"Erreur Spark: {str(e)}")
        st.error(f"‚ùå Erreur Spark: {str(e)}")
        return None

# -------------------------------
# Charger le mod√®le
# -------------------------------
@st.cache_resource
def load_model(_spark, model_path):
    try:
        model_path = Path(model_path)
        metadata_file = model_path / "metadata"

        if not model_path.exists():
            raise FileNotFoundError(f"‚ùå Dossier mod√®le introuvable: {model_path}")
        if not metadata_file.exists():
            raise FileNotFoundError(f"‚ùå Fichier metadata manquant dans: {model_path}")

        model = PipelineModel.load(str(model_path))
        logger.info(f"‚úÖ Mod√®le charg√© depuis: {model_path}")
        return model
    except Exception as e:
        logger.error(f"Erreur chargement mod√®le: {str(e)}")
        raise

# -------------------------------
# Validation inputs
# -------------------------------
def validate_inputs(credit_score, age, tenure, balance, num_products, salary):
    errors = []
    if credit_score < 300 or credit_score > 900:
        errors.append("‚ö†Ô∏è Credit Score entre 300 et 900")
    if age < 18 or age > 100:
        errors.append("‚ö†Ô∏è Age entre 18 et 100 ans")
    if tenure < 0 or tenure > 10:
        errors.append("‚ö†Ô∏è Anciennet√© entre 0 et 10 ans")
    if balance < 0:
        errors.append("‚ö†Ô∏è Solde >= 0")
    if num_products < 1 or num_products > 5:
        errors.append("‚ö†Ô∏è Nombre de produits entre 1 et 5")
    if salary < 0:
        errors.append("‚ö†Ô∏è Salaire >= 0")
    return errors

# -------------------------------
# Pr√©diction
# -------------------------------
def make_prediction(spark, model, input_dict):
    try:
        input_data = [[
            input_dict["CreditScore"],
            input_dict["Age"],
            input_dict["Tenure"],
            input_dict["Balance"],
            input_dict["NumOfProducts"],
            input_dict["HasCrCard"],
            input_dict["IsActiveMember"],
            input_dict["EstimatedSalary"],
            input_dict["Gender"],
            input_dict["Geography"]
        ]]
        columns = ["CreditScore", "Age", "Tenure", "Balance", "NumOfProducts",
                   "HasCrCard", "IsActiveMember", "EstimatedSalary", "Gender", "Geography"]
        input_df = spark.createDataFrame(input_data, columns)
        result_df = model.transform(input_df)
        result = result_df.collect()[0]
        prediction = int(result["prediction"])
        probability = float(result["probability"][prediction])
        return prediction, probability, None
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        return None, None, str(e)

# -------------------------------
# Streamlit Main
# -------------------------------
def main():
    st.set_page_config(page_title="Pr√©diction Attrition Bancaire", layout="wide")
    st.title("üè¶ Pr√©diction d'Attrition Bancaire")

    setup_environment()
    spark = initialize_spark()
    if spark is None:
        st.stop()

    # Chemin mod√®le
    model_path = get_model_path()
    st.sidebar.subheader("üîç Debug Chemin Mod√®le")
    st.sidebar.text(str(model_path))
    if model_path.exists():
        st.sidebar.success("‚úÖ Dossier mod√®le trouv√©")
        files = list(model_path.glob("*"))
        st.sidebar.info(f"üìÅ {len(files)} fichiers dans le mod√®le")
    else:
        st.sidebar.error("‚ùå Dossier mod√®le introuvable")
        st.stop()

    # Charger mod√®le
    try:
        model = load_model(spark, model_path)
        st.sidebar.success("‚úÖ Mod√®le charg√©")
    except Exception as e:
        st.error(f"‚ùå Erreur chargement mod√®le:\n{str(e)}")
        st.stop()

    # Inputs utilisateur
    col1, col2 = st.columns(2)
    with col1:
        credit_score = st.number_input("Credit Score", 300, 900, 650)
        balance = st.number_input("Solde (‚Ç¨)", 0.0, 300000.0, 50000.0, step=1000.0)
        salary = st.number_input("Salaire (‚Ç¨)", 0.0, 200000.0, 50000.0, step=1000.0)
        num_products = st.number_input("Nombre de Produits", 1, 5, 2)
    with col2:
        age = st.number_input("√Çge", 18, 100, 35)
        tenure = st.number_input("Anciennet√©", 0, 10, 3)
        gender = st.selectbox("Genre", ["Male", "Female"])
        geography = st.selectbox("Pays", ["France", "Germany", "Spain"])

    col3, col4 = st.columns(2)
    with col3:
        has_card = st.selectbox("Poss√®de Carte", [0,1])
    with col4:
        is_active = st.selectbox("Membre Actif", [0,1])

    if st.button("üîÆ Pr√©dire"):
        errors = validate_inputs(credit_score, age, tenure, balance, num_products, salary)
        if errors:
            for e in errors:
                st.error(e)
            return

        input_dict = {
            "CreditScore": credit_score,
            "Age": age,
            "Tenure": tenure,
            "Balance": balance,
            "NumOfProducts": num_products,
            "HasCrCard": has_card,
            "IsActiveMember": is_active,
            "EstimatedSalary": salary,
            "Gender": gender,
            "Geography": geography
        }

        with st.spinner("üîÑ Analyse..."):
            prediction, probability, error = make_prediction(spark, model, input_dict)

        if error:
            st.error(f"‚ùå Erreur pr√©diction: {error}")
            return

        st.subheader("üìà R√©sultat")
        if prediction == 1:
            st.error(f"‚ö†Ô∏è Risque √âlev√© - Probabilit√©: {probability:.1%}")
        else:
            st.success(f"‚úÖ Client Fid√®le - Probabilit√©: {probability:.1%}")

if __name__ == "__main__":
    main()
